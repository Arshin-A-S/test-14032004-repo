from __future__ import annotations
import os, argparse, json
from typing import Dict, List
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# Assuming these functions are in your common.py file
from common import (
    BASE_DIR, DATA_DIR, Laplace, read_events,
    groupby_client_counts, aggregate_fedavg, build_stats_section,
    save_model_v2, score_event, time_to_bucket, choose_threshold
)

# CHANGED: Default CSV to the one generated by the new script
DEFAULT_OUT = os.path.join(BASE_DIR, "fl_model_v2.json")
DEFAULT_CSV = os.path.join(DATA_DIR, "synthetic_events.csv") # CHANGED

def one_hot_columns(df: pd.DataFrame, col: str) -> pd.DataFrame:
    return pd.get_dummies(df[col], prefix=col)

def learn_group_weights_logit(train_df: pd.DataFrame) -> Dict[str, float]:
    """
    Learn per-group weights via logistic regression and map to
    non-negative weights that sum to 1 across groups.
    """
    # ADDED: 'department' to the list of features
    X = pd.concat([
        one_hot_columns(train_df, "location"),
        one_hot_columns(train_df, "device"),
        one_hot_columns(train_df, "time_bucket"),
        one_hot_columns(train_df, "department"), # ADDED
    ], axis=1)
    
    # CHANGED: The target variable is now 'is_anomaly'
    y = train_df["is_anomaly"].astype(int).values # CHANGED
    
    if X.shape[1] == 0:
        return {"location": 0.4, "device": 0.3, "time": 0.2, "department": 0.1} # ADDED department

    clf = LogisticRegression(max_iter=200, solver="liblinear")
    clf.fit(X.values, y)
    coefs = clf.coef_

    # Sum absolute coefficients per group
    idx = 0
    group_abs = {}
    # ADDED 'department' to the loop
    for col_group in ["location", "device", "time_bucket", "department"]: # ADDED department
        cols = [c for c in X.columns if c.startswith(col_group + "_")]
        k = len(cols)
        if k == 0:
            group_abs[col_group] = 0.0
            continue
        w = np.sum(np.abs(coefs[idx : idx + k]))
        group_abs[col_group] = float(w)
        idx += k

    # Map to weights dict with keys expected by runtime
    loc_w = group_abs.get("location", 0.0)
    dev_w = group_abs.get("device", 0.0)
    tim_w = group_abs.get("time_bucket", 0.0)
    dept_w = group_abs.get("department", 0.0) # ADDED
    
    s = loc_w + dev_w + tim_w + dept_w # ADDED dept_w
    if s <= 1e-9:
        w = {"location": 0.4, "device": 0.3, "time": 0.2, "department": 0.1} # ADDED department
    else:
        w = {"location": loc_w/s, "device": dev_w/s, "time": tim_w/s, "department": dept_w/s} # ADDED department
    
    eps = 1e-3
    w = {k: float(max(eps, v)) for k,v in w.items()}
    s = sum(w.values())
    return {k: float(v/s) for k,v in w.items()}

def make_scores(df: pd.DataFrame, model: Dict) -> np.ndarray:
    scores = []
    for _, r in df.iterrows():
        # ADDED: Pass 'department' to the scoring function
        scores.append(
            score_event(model,
                        location=r["location"],
                        device=r["device"],
                        time_bucket=r["time_bucket"],
                        department=r["department"]) # ADDED
        )
    return np.array(scores, dtype=float)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--events", default=DEFAULT_CSV, help="CSV with events")
    ap.add_argument("--out", default=DEFAULT_OUT, help="Path to write v2 model")
    ap.add_argument("--val_size", type=float, default=0.25)
    ap.add_argument("--seed", type=int, default=17)
    ap.add_argument("--laplace_success", type=float, default=1.0)
    ap.add_argument("--laplace_fail", type=float, default=1.0)
    ap.add_argument("--target_fpr", type=float, default=0.05) # CHANGED: Target a lower, more realistic FPR
    ap.add_argument("--trim_ratio", type=float, default=0.10)
    args = ap.parse_args()

    df = read_events(args.events)
    
    # CHANGED: Stratify by 'is_anomaly' to ensure balanced train/test sets for anomalies
    train_df, val_df = train_test_split(df, test_size=args.val_size, random_state=args.seed, stratify=df["is_anomaly"])

    # Simulated federated: per-client counts -> FedAvg aggregate
    client_counts_loc = groupby_client_counts(train_df, key="location")
    client_counts_dev = groupby_client_counts(train_df, key="device")
    client_counts_tim = groupby_client_counts(train_df, key="time_bucket")
    client_counts_dept = groupby_client_counts(train_df, key="department") # ADDED

    agg_loc = aggregate_fedavg(client_counts_loc)
    agg_dev = aggregate_fedavg(client_counts_dev)
    agg_tim = aggregate_fedavg(client_counts_tim)
    agg_dept = aggregate_fedavg(client_counts_dept) # ADDED

    stats = build_stats_section(train_df)
    stats["location_stats"] = agg_loc
    stats["device_stats"] = agg_dev
    stats["time_stats"] = agg_tim
    stats["department_stats"] = agg_dept # ADDED

    weights = learn_group_weights_logit(train_df)

    # --- Threshold Calibration ---
    lp = Laplace(args.laplace_success, args.laplace_fail)
    aggregation_cfg = {
        "algorithm": "FedAvg", "robust_aggregation": { "enabled": True, "type": "trimmed_mean", "trim_ratio": float(args.trim_ratio) }
    }
    model = save_model_v2(path=args.out, stats=stats, lp=lp, weights=weights,
                          threshold=0.5, aggregation_cfg=aggregation_cfg, # Dummy threshold
                          validation_meta={"holdout_enabled": True})

    # CHANGED: y_true is now the 'is_anomaly' column from the validation set
    y_true = val_df["is_anomaly"].astype(int).values
    scores = make_scores(val_df, model=model)
    threshold = choose_threshold(y_true, scores, target_fpr=args.target_fpr)

    # --- Final Metrics Calculation ---
    y_pred = (scores >= threshold).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0

    print("\n--- FINAL EVALUATION RESULTS ---")
    print(f"Target FPR:      {args.target_fpr:.2%}")
    print(f"Chosen Threshold:  {threshold:.4f}")
    print(f"Actual FPR:        {fpr:.4%} ({fp}/{fp+tn})")
    print(f"Accuracy:        {accuracy_score(y_true, y_pred):.4%}")
    print(f"Precision:       {precision_score(y_true, y_pred):.4%}")
    print(f"Recall (TPR):      {recall_score(y_true, y_pred):.4%}")
    print("---------------------------------")
    
    try:
        auc = roc_auc_score(y_true, scores)
    except Exception:
        auc = float("nan")

    validation_meta = {
        "holdout_enabled": True,
        "metrics": {
            "val_size": int(len(val_df)),
            "roc_auc_anomaly": float(auc),
            "target_fpr": float(args.target_fpr),
            "chosen_threshold": float(threshold),
            "actual_fpr": float(fpr), # ADDED for clarity
            "accuracy": float(accuracy_score(y_true, y_pred)), # ADDED
            "precision": float(precision_score(y_true, y_pred)), # ADDED
            "recall": float(recall_score(y_true, y_pred)) # ADDED
        }
    }

    # Save final model with calibrated threshold
    save_model_v2(path=args.out, stats=stats, lp=lp, weights=weights,
                  threshold=threshold, aggregation_cfg=aggregation_cfg,
                  validation_meta=validation_meta)

    print(f"\nSaved final model to {args.out}")
    print(json.dumps(validation_meta["metrics"], indent=2))

if __name__ == "__main__":
    main()